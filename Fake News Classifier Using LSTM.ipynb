{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import data to a pandas data frame","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\ntrain_data = train.copy()\ntest_data = test.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"looking to data ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping the NaN values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.set_index('id', drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.shape)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_data.shape)\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking for missing values\ntrain_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping missing values from text columns alone. \ntrain_data[['title', 'author']] = train_data[['title', 'author']].fillna(value = 'Missing')\ntrain_data = train_data.dropna()\ntrain_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"length = []\n[length.append(len(str(text))) for text in train_data['text']]\ntrain_data['length'] = length\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min(train_data['length']), max(train_data['length']), round(sum(train_data['length'])/len(train_data['length']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_data[train_data['length'] < 50])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['text'][train_data['length'] < 50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping the outliers\ntrain_data = train_data.drop(train_data['text'][train_data['length'] < 50].index, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min(train_data['length']), max(train_data['length']), round(sum(train_data['length'])/len(train_data['length']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##importing tensorflow and looking into the version of it\nimport tensorflow as tf\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## import all necessaries\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dropout, RNN, SpatialDropout1D","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## voc size\nvoc_size =  4500","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words = voc_size, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\ntokenizer.fit_on_texts(texts = train_data['text'])\nX = tokenizer.texts_to_sequences(texts = train_data['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now applying padding to make them even shaped.\nX = pad_sequences(sequences = X, maxlen = voc_size, padding = 'pre')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape)\ny = train_data['label'].values\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting the data training data for training and validation.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets create our model......**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"##creating model\nembdding_vector_features = 40\nmodel = Sequential()\nmodel.add(Embedding(voc_size,embdding_vector_features, input_length = 20))\nmodel.add(LSTM(100))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(loss = 'binary_crossentropy',optimizer = 'adam', metrics =  ['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Woohooo!!! lets train our model....","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## model training \nmodel_fit = model.fit(X_train, y_train, epochs = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performance matrix and accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test.shape)\ntest_data = test.copy()\nprint(test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = test_data.set_index('id', drop = True)\ntest_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = test_data.fillna(' ')\nprint(test_data.shape)\ntest_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.fit_on_texts(texts = test_data['text'])\ntest_text = tokenizer.texts_to_sequences(texts = test_data['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_text = pad_sequences(sequences = test_text, maxlen = voc_size, padding = 'pre')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict_classes(test_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test_data['id']), len(pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub = pd.DataFrame()\ndf_sub['id'] = test_data['id']\ndf_sub['label'] = pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_sub.to_csv('submission1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}